{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Aggregate Module Params\n",
    "\n",
    "This notebook should be used as a test for ensuring correct aggregate parameters before aggregate processing.\n",
    "Cells marked with <font color='red'>SET PARAMETERS</font> contain crucial variables that need to be set according to your specific experimental setup and data organization.\n",
    "Please review and modify these variables as needed before proceeding with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Fixed parameters for aggregate module\n",
    "\n",
    "- `CONFIG_FILE_PATH`: Path to a Brieflow config file used during processing. Absolute or relative to where workflows are run from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config/config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lib.shared.file_utils import get_filename, load_parquet_subset\n",
    "from lib.aggregate.cell_data_utils import split_cell_data, channel_combo_subset\n",
    "from lib.aggregate.cell_classification import CellClassifier\n",
    "from lib.aggregate.montage_utils import create_cell_montage, add_filenames\n",
    "from lib.aggregate.filter import (\n",
    "    query_filter,\n",
    "    perturbation_filter,\n",
    "    missing_values_filter,\n",
    "    intensity_filter,\n",
    ")\n",
    "from lib.aggregate.align import (\n",
    "    prepare_alignment_data,\n",
    "    pca_variance_plot,\n",
    "    embed_by_pca,\n",
    "    tvn_on_controls,\n",
    ")\n",
    "from lib.aggregate.aggregate import aggregate\n",
    "from lib.aggregate.eval_aggregate import (\n",
    "    nas_summary,\n",
    "    summarize_cell_data,\n",
    "    plot_feature_distributions,\n",
    ")\n",
    "from lib.shared.configuration_utils import CONFIG_FILE_HEADER\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Testing on subset of data\n",
    "\n",
    "- `TEST_PLATE`: Plate used for testing configuration \n",
    "- `TEST_WELL_1`: First well identifier used for testing configuration\n",
    "- `TEST_WELL_2`: Second well identifier used for testing configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PLATE = None\n",
    "TEST_WELL_1 = None\n",
    "TEST_WELL_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file and determine root path\n",
    "with open(CONFIG_FILE_PATH, \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "ROOT_FP = Path(config[\"all\"][\"root_fp\"])\n",
    "\n",
    "# Load subset of data\n",
    "# Takes ~1 minute\n",
    "merge_final_fp = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_1}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data = load_parquet_subset(merge_final_fp, n_rows=25000)\n",
    "\n",
    "merge_final_fp_2 = (\n",
    "    ROOT_FP\n",
    "    / \"merge\"\n",
    "    / \"parquets\"\n",
    "    / get_filename({\"plate\": TEST_PLATE, \"well\": TEST_WELL_2}, \"merge_final\", \"parquet\")\n",
    ")\n",
    "cell_data_2 = load_parquet_subset(merge_final_fp_2, n_rows=25000)\n",
    "\n",
    "cell_data = pd.concat([cell_data, cell_data_2], ignore_index=True)\n",
    "cell_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cell_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Cell Data Metadata\n",
    "\n",
    "- `METADATA_COLS_FP`: Path to TSV to store metadata cols.\n",
    "- `METADATA_COLS`: Columns in cell data with metadata (use output above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_COLS_FP = \"config/cell_data_metadata_cols.tsv\"\n",
    "METADATA_COLS = [\n",
    "    \"plate\",\n",
    "    \"well\",\n",
    "    \"tile\",\n",
    "    \"cell_0\",\n",
    "    \"i_0\",\n",
    "    \"j_0\",\n",
    "    \"site\",\n",
    "    \"cell_1\",\n",
    "    \"i_1\",\n",
    "    \"j_1\",\n",
    "    \"distance\",\n",
    "    \"fov_distance_0\",\n",
    "    \"fov_distance_1\",\n",
    "    \"sgRNA_0\",\n",
    "    \"gene_symbol_0\",\n",
    "    \"mapped_single_gene\",\n",
    "    \"channels_min\",\n",
    "    \"nucleus_i\",\n",
    "    \"nucleus_j\",\n",
    "    \"nucleus_bounds_0\",\n",
    "    \"nucleus_bounds_1\",\n",
    "    \"nucleus_bounds_2\",\n",
    "    \"nucleus_bounds_3\",\n",
    "    \"cell_i\",\n",
    "    \"cell_j\",\n",
    "    \"cell_bounds_0\",\n",
    "    \"cell_bounds_1\",\n",
    "    \"cell_bounds_2\",\n",
    "    \"cell_bounds_3\",\n",
    "    \"cytoplasm_i\",\n",
    "    \"cytoplasm_j\",\n",
    "    \"cytoplasm_bounds_0\",\n",
    "    \"cytoplasm_bounds_1\",\n",
    "    \"cytoplasm_bounds_2\",\n",
    "    \"cytoplasm_bounds_3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(METADATA_COLS).to_csv(METADATA_COLS_FP, index=False, header=False, sep=\"\\t\")\n",
    "\n",
    "metadata, features = split_cell_data(cell_data, METADATA_COLS)\n",
    "print(metadata.shape, features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Split cells into classes\n",
    "\n",
    "- `CLASSIFIER_PATH`: Path to pickled Python object that can take a cell data dataframe and output cell classes\n",
    "\n",
    "### Evaluate splitting\n",
    "\n",
    "- `COLLAPSE_COLS`: Columns to collapse on when creating a summary of cell counts.\n",
    "- `MONTAGE_CHANNEL`: Channel to use for montage generation. Usually `DAPI`\n",
    "\n",
    "**Note**: You must import necessary packages for the classifier here and add them to `scripts/aggregate/split_datasets.py` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_PATH = None\n",
    "MONTAGE_CHANNEL = None\n",
    "COLLAPSE_COLS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = CellClassifier.load(CLASSIFIER_PATH)\n",
    "metadata, features = classifier.classify_cells(metadata, features)\n",
    "\n",
    "# Create config var for cell classes\n",
    "CELL_CLASSES = list(metadata[\"class\"].unique())\n",
    "\n",
    "# Show cell class counts and distribution\n",
    "print(\"Cell class counts:\")\n",
    "print(metadata[\"class\"].value_counts())\n",
    "\n",
    "print(\"\\nCell class confidences:\")\n",
    "metadata[\"confidence\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_classes = list(metadata[\"class\"].unique()) + [\"all\"]\n",
    "\n",
    "classified_metadata = metadata.copy(deep=True)\n",
    "classified_metadata = add_filenames(classified_metadata, ROOT_FP)\n",
    "\n",
    "# Create a dictionary of DataFrames for each cell class\n",
    "cell_class_dfs = {\n",
    "    cell_class: classified_metadata[classified_metadata[\"class\"] == cell_class]\n",
    "    for cell_class in CELL_CLASSES\n",
    "}\n",
    "\n",
    "# Define sorting directions and titles\n",
    "title_templates = {\n",
    "    True: \"Lowest Confidence {cell_class} Cells - {channel}\",\n",
    "    False: \"Highest Confidence {cell_class} Cells - {channel}\",\n",
    "}\n",
    "\n",
    "# Generate montages dynamically\n",
    "montages, titles = [], []\n",
    "for cell_class, cell_df in cell_class_dfs.items():\n",
    "    for ascending in [True, False]:\n",
    "        montage = create_cell_montage(\n",
    "            cell_data=cell_df,\n",
    "            channels=config[\"phenotype\"][\"channel_names\"],\n",
    "            selection_params={\n",
    "                \"method\": \"sorted\",\n",
    "                \"sort_by\": \"confidence\",\n",
    "                \"ascending\": ascending,\n",
    "            },\n",
    "        )[MONTAGE_CHANNEL]\n",
    "        montages.append(montage)\n",
    "        titles.append(\n",
    "            title_templates[ascending].format(\n",
    "                cell_class=cell_class, channel=MONTAGE_CHANNEL\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Determine figure size dynamically\n",
    "num_rows = len(CELL_CLASSES)\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(10, 3 * num_rows))\n",
    "\n",
    "# Display montages\n",
    "for ax, title, montage in zip(axes.flat, titles, montages):\n",
    "    ax.imshow(montage, cmap=\"gray\")\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "print(\"Montages of cell classes:\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Split cell data summary:\")\n",
    "summary_df = summarize_cell_data(metadata, CELL_CLASSES, COLLAPSE_COLS)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Aggregate by channel combos\n",
    "\n",
    "- `CHANNEL_COMBOS`: Combinations of channels to aggregate by.\n",
    "- `AGGREGATE_COMBO_FP`: Location of aggregate combinations dataframe.\n",
    "- `TEST_CELL_CLASS`: Cell class to configure aggregate params with. Can be any of the cell classes or `all`.\n",
    "- `TEST_CHANNEL_COMBO`: Channel combo to configure aggregate params with. Can be any of the channel combos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_COMBOS = None\n",
    "AGGREGATE_COMBO_FP = \"config/aggregate_combo.tsv\"\n",
    "\n",
    "TEST_CELL_CLASS = None\n",
    "TEST_CHANNEL_COMBO = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine cell classes and channel combos\n",
    "channel_combos = [\"_\".join(combo) for combo in CHANNEL_COMBOS]\n",
    "\n",
    "# Generate aggregate wildcard combos\n",
    "MERGE_COMBO_FP = Path(config[\"merge\"][\"merge_combo_fp\"])\n",
    "merge_wildcard_combos = pd.read_csv(MERGE_COMBO_FP, sep=\"\\t\")\n",
    "\n",
    "# Generate full combinations\n",
    "aggregate_wildcard_combos = pd.DataFrame(\n",
    "    product(\n",
    "        merge_wildcard_combos.itertuples(index=False, name=None),\n",
    "        cell_classes,\n",
    "        channel_combos,\n",
    "    ),\n",
    "    columns=[\"plate_well\", \"cell_class\", \"channel_combo\"],\n",
    ")\n",
    "aggregate_wildcard_combos[[\"plate\", \"well\"]] = pd.DataFrame(aggregate_wildcard_combos[\"plate_well\"].tolist(), index=aggregate_wildcard_combos.index)\n",
    "aggregate_wildcard_combos = aggregate_wildcard_combos.drop(columns=\"plate_well\")\n",
    "\n",
    "# Save aggregate wildcard combos\n",
    "aggregate_wildcard_combos.to_csv(AGGREGATE_COMBO_FP, sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Aggregate wildcard combos:\")\n",
    "aggregate_wildcard_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset cell class\n",
    "if TEST_CELL_CLASS != \"all\":\n",
    "    cell_class_mask = metadata[\"class\"] == TEST_CELL_CLASS\n",
    "    metadata = metadata[cell_class_mask]\n",
    "    features = features[cell_class_mask]\n",
    "\n",
    "# subset features\n",
    "all_channels = config[\"phenotype\"][\"channel_names\"]\n",
    "features = channel_combo_subset(features, TEST_CHANNEL_COMBO, all_channels)\n",
    "\n",
    "# copy metadata and features for later eval\n",
    "dataset_metadata = metadata.copy()\n",
    "dataset_features = features.copy()\n",
    "\n",
    "# preview metadata and features\n",
    "display(metadata)\n",
    "display(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Perturbation filtering\n",
    "\n",
    "- `FILTER_QUERIES`: Queries to use for custom filtering; ex: `[\"mapped_single_gene == False\", \"cell_quality_score > 0.8\"]`. Can be left as `None` for no filtering.\n",
    "- `PERTURBATION_NAME_COL`: Name of column used to identify perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_QUERIES = None\n",
    "PERTURBATION_NAME_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, features = query_filter(metadata, features, FILTER_QUERIES)\n",
    "\n",
    "metadata, features = perturbation_filter(\n",
    "    metadata, features, PERTURBATION_NAME_COL\n",
    ")\n",
    "print(f\"Unique populations: {metadata[PERTURBATION_NAME_COL].nunique()}\")\n",
    "\n",
    "sumamry_df, fig = nas_summary(features)\n",
    "sumamry_df[sumamry_df[\"percent_na\"] > 0.1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Missing value filtering\n",
    "\n",
    "- `DROP_COLS_THRESHOLD`: Threshold of NA values above which an entire column is dropped. Usually `0.1`\n",
    "- `DROP_ROWS_THRESHOLD`: Threshold of NA values above which an entire row is dropped. Usually `0.01`\n",
    "- `IMPUTE`: Whether or not to impute remaining missing values. Usually `True`\n",
    "\n",
    "**Note**: All NAs must be dropped or imputed to perform feature alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_THRESHOLD = None\n",
    "DROP_ROWS_THRESHOLD = None\n",
    "IMPUTE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by missing values\n",
    "metadata, features = missing_values_filter(\n",
    "    metadata,\n",
    "    features,\n",
    "    drop_cols_threshold=DROP_COLS_THRESHOLD,\n",
    "    drop_rows_threshold=DROP_ROWS_THRESHOLD,\n",
    "    impute=True,\n",
    ")\n",
    "\n",
    "metadata.shape, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Intensity filtering\n",
    "\n",
    "- `CONTAMINATION`: Expected proportion of outliers in dataset. Usually `0.01`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAMINATION = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by intensity outliers\n",
    "metadata, features = intensity_filter(\n",
    "    metadata,\n",
    "    features,\n",
    "    config[\"phenotype\"][\"channel_names\"],\n",
    "    CONTAMINATION,\n",
    ")\n",
    "\n",
    "metadata.shape, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Prepare alignment data\n",
    "\n",
    "- `BATCH_COLS`: Which columns of metadata have batch-specific information. Usually `[\"plate\", \"well\"]`.\n",
    "- `CONTROL_KEY`: Name of perturbation in `PERTURBATION_NAME_COL` that indicates a control cell.\n",
    "\n",
    "The following parameter is only needed if you want your controls to have different perturbation names. Otherwise, can leave this as `None`.\n",
    "- `PERTURBATION_ID_COL`: Name of column that identifies unique perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_COLS = None\n",
    "CONTROL_KEY = None\n",
    "PERTURBATION_ID_COL = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, metadata = prepare_alignment_data(\n",
    "    metadata, features, BATCH_COLS, PERTURBATION_NAME_COL, CONTROL_KEY, PERTURBATION_ID_COL\n",
    ")\n",
    "\n",
    "n_components, fig = pca_variance_plot(\n",
    "    features, variance_threshold=0.99\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>SET PARAMETERS</font>\n",
    "\n",
    "### Align and aggregate\n",
    "\n",
    "- `VARIANCE_OR_NCOMP`: Variance or number of components to keep after PCA.\n",
    "    Defaults to 128 (n_components). If between 0 and 1, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified. Note that we use 50,000 cells to estimate number of features if a percentage is specified.\n",
    "    If 1, a single component is kept, and if None, all components are kept.\n",
    "    `0.99` is recommended method.\n",
    "- `AGG_METHOD`: Method used to aggregate features. Can be `mean` or `median`. Usually `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_OR_NCOMP = None\n",
    "AGG_METHOD = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = embed_by_pca(\n",
    "    features, metadata, variance_or_ncomp=VARIANCE_OR_NCOMP, batch_col=\"batch_values\"\n",
    ")\n",
    "\n",
    "tvn_normalized = tvn_on_controls(\n",
    "    pca_embeddings, metadata, PERTURBATION_NAME_COL, CONTROL_KEY, \"batch_values\"\n",
    ")\n",
    "\n",
    "aggregated_embeddings, aggregated_metadata = aggregate(\n",
    "    tvn_normalized, metadata, PERTURBATION_NAME_COL, AGG_METHOD\n",
    ")\n",
    "\n",
    "feature_columns = [f\"PC_{i}\" for i in range(tvn_normalized.shape[1])]\n",
    "\n",
    "tvn_normalized_df = pd.DataFrame(\n",
    "    tvn_normalized, index=metadata.index, columns=feature_columns\n",
    ")\n",
    "aligned_cell_data = pd.concat([metadata, tvn_normalized_df], axis=1)\n",
    "\n",
    "aggregated_embeddings_df = pd.DataFrame(\n",
    "    aggregated_embeddings, index=aggregated_metadata.index, columns=feature_columns\n",
    ")\n",
    "aggregated_cell_data = (\n",
    "    pd.concat([aggregated_metadata, aggregated_embeddings_df], axis=1)\n",
    "    .sort_values(\"cell_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_cols = [col for col in dataset_features.columns if (\"cell_\" in col and col.endswith(\"_mean\"))]\n",
    "pc_cols = [col for col in aggregated_cell_data.columns if col.startswith(\"PC_\")]\n",
    "aligned_feature_cols = random.sample(pc_cols, k=min(len(original_feature_cols), len(pc_cols)))\n",
    "\n",
    "original_cell_data = pd.concat([dataset_metadata, dataset_features], axis=1)\n",
    "original_cell_data\n",
    "\n",
    "feature_distributions_fig = plot_feature_distributions(\n",
    "    original_feature_cols,\n",
    "    original_cell_data,\n",
    "    aligned_feature_cols,\n",
    "    aligned_cell_data,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add aggregate parameters to config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add aggregate section\n",
    "config[\"aggregate\"] = {\n",
    "    \"metadata_cols_fp\": METADATA_COLS_FP,\n",
    "    \"collapse_cols\": COLLAPSE_COLS,\n",
    "    \"classifier_path\": CLASSIFIER_PATH,\n",
    "    \"aggregate_combo_fp\": AGGREGATE_COMBO_FP,\n",
    "    \"filter_queries\": FILTER_QUERIES,\n",
    "    \"perturbation_name_col\": PERTURBATION_NAME_COL,\n",
    "    \"drop_cols_threshold\": DROP_COLS_THRESHOLD,\n",
    "    \"drop_rows_threshold\": DROP_ROWS_THRESHOLD,\n",
    "    \"impute\": IMPUTE,\n",
    "    \"contamination\": CONTAMINATION,\n",
    "    \"batch_cols\": BATCH_COLS,\n",
    "    \"control_key\": CONTROL_KEY,\n",
    "    \"perturbation_id_col\": PERTURBATION_ID_COL,\n",
    "    \"variance_or_ncomp\": VARIANCE_OR_NCOMP,\n",
    "    \"agg_method\": AGG_METHOD,\n",
    "}\n",
    "\n",
    "# Write the updated configuration\n",
    "with open(CONFIG_FILE_PATH, \"w\") as config_file:\n",
    "    # Write the introductory comments\n",
    "    config_file.write(CONFIG_FILE_HEADER)\n",
    "\n",
    "    # Dump the updated YAML structure, keeping markdown comments for sections\n",
    "    yaml.dump(config, config_file, default_flow_style=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brieflow_main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
